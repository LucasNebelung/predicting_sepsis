{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_FIT: /teamspace/studios/this_studio/detecting_Sepsis/data/raw_CSV/train_fit.csv\n",
      "TRAIN_THRESH: /teamspace/studios/this_studio/detecting_Sepsis/data/raw_CSV/train_thresh.csv\n",
      "TEST: /teamspace/studios/this_studio/detecting_Sepsis/data/raw_CSV/test.csv\n",
      "LOW_OUT_DIR: /teamspace/studios/this_studio/detecting_Sepsis/data/Low_Preproc_NoFe_CSV\n",
      "HIGH_OUT_DIR: /teamspace/studios/this_studio/detecting_Sepsis/data/High_Preproc_NoFe_CSV\n"
     ]
    }
   ],
   "source": [
    "# Project root\n",
    "BASE_DIR = Path(\"/teamspace/studios/this_studio/detecting_Sepsis\")\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_CSV_DIR  = DATA_DIR / \"raw_CSV\"\n",
    "LOW_DIR      = DATA_DIR / \"Low_Preproc_NoFe_CSV\"\n",
    "HIGH_DIR     = DATA_DIR / \"High_Preproc_NoFe_CSV\"\n",
    "\n",
    "for d in [RAW_CSV_DIR, LOW_DIR, HIGH_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Raw inputs (your current files)\n",
    "TRAIN_FIT_CSV    = RAW_CSV_DIR / \"train_fit.csv\"\n",
    "TRAIN_THRESH_CSV = RAW_CSV_DIR / \"train_thresh.csv\"\n",
    "TEST_CSV         = RAW_CSV_DIR / \"test.csv\"\n",
    "\n",
    "# Column conventions\n",
    "PATIENT_COL = \"Patient_ID\"\n",
    "TIME_COL    = \"ICULOS\"\n",
    "LABEL_COL   = \"SepsisLabel\"\n",
    "\n",
    "# HIGH recency settings (match your loader)\n",
    "RECENCY_DECAY = 0.9\n",
    "NO_RECENCY_COLS = {\"Age\",\"Gender\",\"Unit1\",\"Unit2\",\"HospAdmTime\",\"ICULOS\"}\n",
    "\n",
    "print(\"TRAIN_FIT:\", TRAIN_FIT_CSV)\n",
    "print(\"TRAIN_THRESH:\", TRAIN_THRESH_CSV)\n",
    "print(\"TEST:\", TEST_CSV)\n",
    "print(\"LOW_OUT_DIR:\", LOW_DIR)\n",
    "print(\"HIGH_OUT_DIR:\", HIGH_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Drop accidental index columns like Unnamed: 0\n",
    "    df = df.loc[:, ~df.columns.str.contains(r\"^Unnamed\")]\n",
    "    return df\n",
    "\n",
    "def get_feature_cols(df: pd.DataFrame) -> list[str]:\n",
    "    # Features are everything except patient/time/label\n",
    "    drop_cols = {PATIENT_COL, TIME_COL, LABEL_COL}\n",
    "    return [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "def recency_from_missing(missing_sub: np.ndarray, decay: float = 0.9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    missing_sub: bool [T, F] where True = missing at time t\n",
    "    recency[t] = decay*recency[t-1], set to 1.0 when observed at t\n",
    "    \"\"\"\n",
    "    T, F = missing_sub.shape\n",
    "    rec = np.zeros((T, F), dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        obs = ~missing_sub[t]\n",
    "        if t == 0:\n",
    "            rec[t, obs] = 1.0\n",
    "        else:\n",
    "            rec[t] = rec[t-1] * decay\n",
    "            rec[t, obs] = 1.0\n",
    "    return rec\n",
    "\n",
    "def make_low_preproc(df: pd.DataFrame, feature_cols: list[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out = out.sort_values([PATIENT_COL, TIME_COL]).reset_index(drop=True)\n",
    "    out[feature_cols] = out[feature_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    out[feature_cols] = (\n",
    "        out.groupby(PATIENT_COL, sort=False)[feature_cols]\n",
    "           .ffill()\n",
    "           .fillna(0.0)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def make_high_preproc(df: pd.DataFrame, feature_cols: list[str], decay: float = 0.9) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out = out.sort_values([PATIENT_COL, TIME_COL]).reset_index(drop=True)\n",
    "\n",
    "    # numeric coercion (non-numeric -> NaN)\n",
    "    out[feature_cols] = out[feature_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # dynamic cols that get recency\n",
    "    dyn_cols = [c for c in feature_cols if c not in NO_RECENCY_COLS]\n",
    "    dyn_idx = [feature_cols.index(c) for c in dyn_cols]\n",
    "\n",
    "    recency_parts = []\n",
    "    for pid, g in out.groupby(PATIENT_COL, sort=False):\n",
    "        X_raw = g[feature_cols].to_numpy(dtype=np.float32, copy=True)\n",
    "        missing = np.isnan(X_raw)\n",
    "\n",
    "        if len(dyn_idx) == 0:\n",
    "            rec = np.zeros((len(g), 0), dtype=np.float32)\n",
    "        else:\n",
    "            rec = recency_from_missing(missing[:, dyn_idx], decay=decay)\n",
    "\n",
    "        rec_df = pd.DataFrame(\n",
    "            rec,\n",
    "            index=g.index,\n",
    "            columns=[f\"recency_{c}\" for c in dyn_cols],\n",
    "        )\n",
    "        recency_parts.append(rec_df)\n",
    "\n",
    "    rec_all = pd.concat(recency_parts).sort_index()\n",
    "\n",
    "    # Apply LOW value preprocessing (ffill then 0)\n",
    "    out[feature_cols] = (\n",
    "        out.groupby(PATIENT_COL, sort=False)[feature_cols]\n",
    "           .ffill()\n",
    "           .fillna(0.0)\n",
    "    )\n",
    "\n",
    "    # Append recency columns\n",
    "    out = pd.concat([out, rec_all], axis=1)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing: train_fit.csv ===\n",
      "Rows: 1180166 | Feature cols: 40\n",
      "Wrote LOW : train_fit_LOW_PREPROC_NO_FE.csv shape: (1180166, 43)\n",
      "Wrote HIGH: train_fit_HIGH_PREPROC_NO_FE.csv shape: (1180166, 78)\n",
      "\n",
      "=== Processing: train_thresh.csv ===\n",
      "Rows: 61120 | Feature cols: 40\n",
      "Wrote LOW : train_thresh_LOW_PREPROC_NO_FE.csv shape: (61120, 43)\n",
      "Wrote HIGH: train_thresh_HIGH_PREPROC_NO_FE.csv shape: (61120, 78)\n",
      "\n",
      "=== Processing: test.csv ===\n",
      "Rows: 310924 | Feature cols: 40\n",
      "Wrote LOW : test_LOW_PREPROC_NO_FE.csv shape: (310924, 43)\n",
      "Wrote HIGH: test_HIGH_PREPROC_NO_FE.csv shape: (310924, 78)\n",
      "\n",
      "DONE preprocessing.\n",
      "train_fit_high: /teamspace/studios/this_studio/detecting_Sepsis/data/High_Preproc_NoFe_CSV/train_fit_HIGH_PREPROC_NO_FE.csv\n",
      "train_thresh_high: /teamspace/studios/this_studio/detecting_Sepsis/data/High_Preproc_NoFe_CSV/train_thresh_HIGH_PREPROC_NO_FE.csv\n",
      "test_high: /teamspace/studios/this_studio/detecting_Sepsis/data/High_Preproc_NoFe_CSV/test_HIGH_PREPROC_NO_FE.csv\n"
     ]
    }
   ],
   "source": [
    "def preprocess_and_write(in_path: Path, low_dir: Path, high_dir: Path):\n",
    "    print(f\"\\n=== Processing: {in_path.name} ===\")\n",
    "    df = load_raw_csv(in_path)\n",
    "\n",
    "    # basic checks\n",
    "    for c in [PATIENT_COL, TIME_COL, LABEL_COL]:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"{in_path.name}: missing required column '{c}'\")\n",
    "\n",
    "    feature_cols = get_feature_cols(df)\n",
    "    print(\"Rows:\", len(df), \"| Feature cols:\", len(feature_cols))\n",
    "\n",
    "    df_low = make_low_preproc(df, feature_cols)\n",
    "    df_high = make_high_preproc(df, feature_cols, decay=RECENCY_DECAY)\n",
    "\n",
    "    low_out  = low_dir  / in_path.name.replace(\".csv\", \"_LOW_PREPROC_NO_FE.csv\")\n",
    "    high_out = high_dir / in_path.name.replace(\".csv\", \"_HIGH_PREPROC_NO_FE.csv\")\n",
    "\n",
    "    df_low.to_csv(low_out, index=False)\n",
    "    df_high.to_csv(high_out, index=False)\n",
    "\n",
    "    print(\"Wrote LOW :\", low_out.name,  \"shape:\", df_low.shape)\n",
    "    print(\"Wrote HIGH:\", high_out.name, \"shape:\", df_high.shape)\n",
    "    return low_out, high_out\n",
    "\n",
    "train_fit_low, train_fit_high = preprocess_and_write(TRAIN_FIT_CSV, LOW_DIR, HIGH_DIR)\n",
    "train_thresh_low, train_thresh_high = preprocess_and_write(TRAIN_THRESH_CSV, LOW_DIR, HIGH_DIR)\n",
    "test_low, test_high = preprocess_and_write(TEST_CSV, LOW_DIR, HIGH_DIR)\n",
    "\n",
    "print(\"\\nDONE preprocessing.\")\n",
    "print(\"train_fit_high:\", train_fit_high)\n",
    "print(\"train_thresh_high:\", train_thresh_high)\n",
    "print(\"test_high:\", test_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote train_fit_LOW_PREPROC_WITH_MISSING.csv | added 40 miss_* columns\n",
      "Wrote train_thresh_LOW_PREPROC_WITH_MISSING.csv | added 40 miss_* columns\n",
      "Wrote test_LOW_PREPROC_WITH_MISSING.csv | added 40 miss_* columns\n",
      "\n",
      "DONE → use CSVs from: /teamspace/studios/this_studio/detecting_Sepsis/data/Low_Preproc_WithMissing_CSV\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Create LOW + Missingness CSVs (from RAW + LOW)\n",
    "# ============================================================\n",
    "\n",
    "OUT_DIR = DATA_DIR / \"Low_Preproc_WithMissing_CSV\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def drop_unnamed(df):\n",
    "    return df.loc[:, ~df.columns.str.contains(r\"^Unnamed\")]\n",
    "\n",
    "def add_missingness(raw_path: Path, low_path: Path, out_path: Path):\n",
    "    raw = drop_unnamed(pd.read_csv(raw_path))\n",
    "    low = drop_unnamed(pd.read_csv(low_path))\n",
    "\n",
    "    # feature columns = LOW features (excluding id/time/label)\n",
    "    drop_cols = {PATIENT_COL, TIME_COL, LABEL_COL}\n",
    "    feat_cols = [c for c in low.columns if c not in drop_cols]\n",
    "\n",
    "    # ensure RAW has same feature columns\n",
    "    for c in feat_cols:\n",
    "        if c not in raw.columns:\n",
    "            raw[c] = np.nan\n",
    "\n",
    "    raw = raw.sort_values([PATIENT_COL, TIME_COL]).reset_index(drop=True)\n",
    "    low = low.sort_values([PATIENT_COL, TIME_COL]).reset_index(drop=True)\n",
    "\n",
    "    # missingness from RAW (before ffill!)\n",
    "    miss = raw[feat_cols].apply(pd.to_numeric, errors=\"coerce\").isna().astype(np.int8)\n",
    "    miss.columns = [f\"miss_{c}\" for c in miss.columns]\n",
    "\n",
    "    miss_df = pd.concat([raw[[PATIENT_COL, TIME_COL]], miss], axis=1)\n",
    "\n",
    "    merged = low.merge(\n",
    "        miss_df,\n",
    "        on=[PATIENT_COL, TIME_COL],\n",
    "        how=\"left\",\n",
    "        validate=\"one_to_one\"\n",
    "    )\n",
    "\n",
    "    if len(merged) != len(low):\n",
    "        raise RuntimeError(\"Row mismatch after merge\")\n",
    "\n",
    "    merged.to_csv(out_path, index=False)\n",
    "    print(f\"Wrote {out_path.name} | added {miss.shape[1]} miss_* columns\")\n",
    "\n",
    "# --- run for all splits ---\n",
    "add_missingness(\n",
    "    RAW_CSV_DIR / \"train_fit.csv\",\n",
    "    LOW_DIR / \"train_fit_LOW_PREPROC_NO_FE.csv\",\n",
    "    OUT_DIR / \"train_fit_LOW_PREPROC_WITH_MISSING.csv\",\n",
    ")\n",
    "\n",
    "add_missingness(\n",
    "    RAW_CSV_DIR / \"train_thresh.csv\",\n",
    "    LOW_DIR / \"train_thresh_LOW_PREPROC_NO_FE.csv\",\n",
    "    OUT_DIR / \"train_thresh_LOW_PREPROC_WITH_MISSING.csv\",\n",
    ")\n",
    "\n",
    "add_missingness(\n",
    "    RAW_CSV_DIR / \"test.csv\",\n",
    "    LOW_DIR / \"test_LOW_PREPROC_NO_FE.csv\",\n",
    "    OUT_DIR / \"test_LOW_PREPROC_WITH_MISSING.csv\",\n",
    ")\n",
    "\n",
    "print(\"\\nDONE → use CSVs from:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote train_fit_high_preproc_Mean_Impute.csv | shape=(1180166, 78)\n",
      "Wrote train_thresh_high_preproc_Mean_Impute.csv | shape=(61120, 78)\n",
      "Wrote test_high_preproc_Mean_Impute.csv | shape=(310924, 78)\n",
      "\n",
      "DONE → use CSVs from: /teamspace/studios/this_studio/detecting_Sepsis/data/High_Preproc_TrainMeanImpute_CSV\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Create HIGH-preproc CSVs (from RAW) with:\n",
    "#   - per-patient forward-fill\n",
    "#   - recency_* features (computed from RAW missingness BEFORE ffill)\n",
    "#   - remaining NaNs imputed with GLOBAL TRAINING MEAN (fit on RAW train_fit)\n",
    "#\n",
    "# Writes 3 CSVs:\n",
    "#   - train_fit_high_preproc_Mean_Impute.csv\n",
    "#   - train_thresh_high_preproc_Mean_Impute.csv\n",
    "#   - test_high_preproc_Mean_Impute.csv\n",
    "# ============================================================\n",
    "\n",
    "OUT_DIR = DATA_DIR / \"High_Preproc_TrainMeanImpute_CSV\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def drop_unnamed(df):\n",
    "    return df.loc[:, ~df.columns.str.contains(r\"^Unnamed\")]\n",
    "\n",
    "def get_feat_cols(df):\n",
    "    drop_cols = {PATIENT_COL, TIME_COL, LABEL_COL}\n",
    "    return [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "def compute_recency_from_missing(missing_mask: np.ndarray, decay: float = 0.9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    missing_mask: shape (T, D) boolean, True if value is missing at time t for feature d\n",
    "    returns: recency in (0,1], with 1 when observed at t, else decay*previous\n",
    "    \"\"\"\n",
    "    T, D = missing_mask.shape\n",
    "    rec = np.zeros((T, D), dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        if t == 0:\n",
    "            rec[t] = np.where(~missing_mask[t], 1.0, 0.0)\n",
    "        else:\n",
    "            rec[t] = np.where(~missing_mask[t], 1.0, decay * rec[t - 1])\n",
    "    return rec\n",
    "\n",
    "def make_high_preproc_with_trainmean_impute(\n",
    "    raw_path: Path,\n",
    "    out_path: Path,\n",
    "    feat_cols: list,\n",
    "    train_means: pd.Series,\n",
    "    decay: float = 0.9,\n",
    "):\n",
    "    raw = drop_unnamed(pd.read_csv(raw_path))\n",
    "    raw = raw.sort_values([PATIENT_COL, TIME_COL]).reset_index(drop=True)\n",
    "\n",
    "    # ensure all feat_cols exist (in case columns differ across splits)\n",
    "    for c in feat_cols:\n",
    "        if c not in raw.columns:\n",
    "            raw[c] = np.nan\n",
    "\n",
    "    # coerce to numeric for feature columns\n",
    "    raw[feat_cols] = raw[feat_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # dynamic cols for recency\n",
    "    dyn_cols = [c for c in feat_cols if c not in NO_RECENCY_COLS]\n",
    "\n",
    "    recency_parts = []\n",
    "    # compute recency per patient from RAW missingness pattern\n",
    "    for pid, g in raw.groupby(PATIENT_COL, sort=False):\n",
    "        miss = g[dyn_cols].isna().to_numpy(dtype=bool, copy=False)\n",
    "        rec = compute_recency_from_missing(miss, decay=decay)\n",
    "        rec_df = pd.DataFrame(rec, index=g.index, columns=[f\"recency_{c}\" for c in dyn_cols])\n",
    "        recency_parts.append(rec_df)\n",
    "\n",
    "    rec_all = pd.concat(recency_parts, axis=0).sort_index()\n",
    "\n",
    "    # HIGH values: forward fill within patient\n",
    "    high = raw.copy()\n",
    "    high[feat_cols] = high.groupby(PATIENT_COL, sort=False)[feat_cols].ffill()\n",
    "\n",
    "    # remaining NaNs -> GLOBAL TRAINING MEAN (fit on RAW train_fit)\n",
    "    # (align ensures columns match even if order differs)\n",
    "    high[feat_cols] = high[feat_cols].fillna(train_means.reindex(feat_cols))\n",
    "\n",
    "    # append recency columns\n",
    "    high = pd.concat([high, rec_all], axis=1)\n",
    "\n",
    "    high.to_csv(out_path, index=False)\n",
    "    print(f\"Wrote {out_path.name} | shape={high.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Fit GLOBAL TRAINING MEANS on RAW train_fit\n",
    "# ------------------------------------------------------------\n",
    "raw_train_fit = drop_unnamed(pd.read_csv(TRAIN_FIT_CSV))\n",
    "raw_train_fit = raw_train_fit.sort_values([PATIENT_COL, TIME_COL]).reset_index(drop=True)\n",
    "\n",
    "feat_cols = get_feat_cols(raw_train_fit)\n",
    "\n",
    "tmp = raw_train_fit.copy()\n",
    "tmp[feat_cols] = tmp[feat_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "train_means = tmp[feat_cols].mean(axis=0, skipna=True)\n",
    "\n",
    "# if a feature is entirely NaN in train_fit -> mean is NaN; fall back to 0.0 (change if you want)\n",
    "train_means = train_means.fillna(0.0)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Create HIGH-preproc splits with train-mean imputation\n",
    "# ------------------------------------------------------------\n",
    "make_high_preproc_with_trainmean_impute(\n",
    "    raw_path=TRAIN_FIT_CSV,\n",
    "    out_path=OUT_DIR / \"train_fit_high_preproc_Mean_Impute.csv\",\n",
    "    feat_cols=feat_cols,\n",
    "    train_means=train_means,\n",
    "    decay=RECENCY_DECAY if \"RECENCY_DECAY\" in globals() else 0.9,\n",
    ")\n",
    "\n",
    "make_high_preproc_with_trainmean_impute(\n",
    "    raw_path=TRAIN_THRESH_CSV,\n",
    "    out_path=OUT_DIR / \"train_thresh_high_preproc_Mean_Impute.csv\",\n",
    "    feat_cols=feat_cols,\n",
    "    train_means=train_means,\n",
    "    decay=RECENCY_DECAY if \"RECENCY_DECAY\" in globals() else 0.9,\n",
    ")\n",
    "\n",
    "make_high_preproc_with_trainmean_impute(\n",
    "    raw_path=TEST_CSV,\n",
    "    out_path=OUT_DIR / \"test_high_preproc_Mean_Impute.csv\",\n",
    "    feat_cols=feat_cols,\n",
    "    train_means=train_means,\n",
    "    decay=RECENCY_DECAY if \"RECENCY_DECAY\" in globals() else 0.9,\n",
    ")\n",
    "\n",
    "print(\"\\nDONE → use CSVs from:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample pid: 11\n",
      "Rows: 34\n",
      "ICULOS head: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "ICULOS tail: [26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check: pick one patient from train_thresh_high and verify monotonic time ordering\n",
    "df_check = pd.read_csv(train_thresh_high)\n",
    "pid = int(df_check[PATIENT_COL].iloc[0])\n",
    "g = df_check[df_check[PATIENT_COL] == pid].sort_values(TIME_COL)\n",
    "\n",
    "print(\"Sample pid:\", pid)\n",
    "print(\"Rows:\", len(g))\n",
    "print(\"ICULOS head:\", g[TIME_COL].head(10).tolist())\n",
    "print(\"ICULOS tail:\", g[TIME_COL].tail(10).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_fit_HIGH_PREPROC_NO_FE_with_signatures.csv | shape=(1180166, 220) | base=75 hc=25 sig=110 csig=7\n",
      "Saved: train_thresh_HIGH_PREPROC_NO_FE_with_signatures.csv | shape=(61120, 220) | base=75 hc=25 sig=110 csig=7\n",
      "Saved: test_HIGH_PREPROC_NO_FE_with_signatures.csv | shape=(310924, 220) | base=75 hc=25 sig=110 csig=7\n",
      "\n",
      "Signature feature run complete.\n",
      "IN_DIR: /teamspace/studios/this_studio/detecting_Sepsis/data/High_Preproc_NoFe_CSV\n",
      "OUT_DIR: /teamspace/studios/this_studio/detecting_Sepsis/data/SignatureFeatureSets/low_FE__orig1_hc1_sig1_csig1_lb4_so2_cso1_ll0\n",
      "RUN_DIR_NAME: low_FE__orig1_hc1_sig1_csig1_lb4_so2_cso1_ll0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Signature feature generation from HIGH preprocessed CSVs\n",
    "# ============================================================\n",
    "# This cell appends signature-style features and writes a run-specific folder under data/SignatureFeatureSets/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import iisignature\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\n",
    "        \"iisignature is required for this cell. Install with: pip install iisignature\"\n",
    "    ) from exc\n",
    "\n",
    "# ---------- Config ----------\n",
    "FE_PRESET = \"low_FE\"  # one of: low_FE, balanced_FE, rich_FE\n",
    "USE_ORIGINAL_VARS = True\n",
    "USE_HANDCRAFTED_VARS = True\n",
    "USE_SIGNATURE_VARS = True\n",
    "USE_CUMSIGNATURE_VARS = True\n",
    "USE_LEAD_LAG = False\n",
    "\n",
    "FE_PRESETS = {\n",
    "    \"low_FE\": {\"lookback\": 4, \"sig_order\": 2, \"csig_order\": 1},\n",
    "    \"balanced_FE\": {\"lookback\": 6, \"sig_order\": 2, \"csig_order\": 3},\n",
    "    \"rich_FE\": {\"lookback\": 7, \"sig_order\": 3, \"csig_order\": 3},\n",
    "}\n",
    "if FE_PRESET not in FE_PRESETS:\n",
    "    raise ValueError(f\"Unknown FE_PRESET={FE_PRESET}. Choose from {list(FE_PRESETS)}\")\n",
    "\n",
    "cfg = FE_PRESETS[FE_PRESET]\n",
    "LOOKBACK = int(cfg[\"lookback\"])\n",
    "SIG_ORDER = int(cfg[\"sig_order\"])\n",
    "CSIG_ORDER = int(cfg[\"csig_order\"])\n",
    "\n",
    "# Robust root resolution (works when earlier BASE_DIR is unavailable)\n",
    "try:\n",
    "    ROOT_DATA_DIR = DATA_DIR\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    ROOT_DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "IN_DIR = ROOT_DATA_DIR / \"High_Preproc_NoFe_CSV\"\n",
    "OUT_ROOT = ROOT_DATA_DIR / \"SignatureFeatureSets\"\n",
    "\n",
    "RUN_DIR_NAME = (\n",
    "    f\"{FE_PRESET}__orig{int(USE_ORIGINAL_VARS)}_hc{int(USE_HANDCRAFTED_VARS)}_\"\n",
    "    f\"sig{int(USE_SIGNATURE_VARS)}_csig{int(USE_CUMSIGNATURE_VARS)}_\"\n",
    "    f\"lb{LOOKBACK}_so{SIG_ORDER}_cso{CSIG_ORDER}_ll{int(USE_LEAD_LAG)}\"\n",
    ")\n",
    "OUT_DIR = OUT_ROOT / RUN_DIR_NAME\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PATIENT_COL = \"Patient_ID\"\n",
    "TIME_COL = \"ICULOS\"\n",
    "LABEL_COL = \"SepsisLabel\"\n",
    "\n",
    "INPUT_FILES = {\n",
    "    \"train_fit_HIGH_PREPROC_NO_FE.csv\": \"train_fit_HIGH_PREPROC_NO_FE_with_signatures.csv\",\n",
    "    \"train_thresh_HIGH_PREPROC_NO_FE.csv\": \"train_thresh_HIGH_PREPROC_NO_FE_with_signatures.csv\",\n",
    "    \"test_HIGH_PREPROC_NO_FE.csv\": \"test_HIGH_PREPROC_NO_FE_with_signatures.csv\",\n",
    "}\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def safe_num(df: pd.DataFrame, col):\n",
    "    if col is None:\n",
    "        return pd.Series(np.nan, index=df.index, dtype=float)\n",
    "    return pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "def add_handcrafted_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    hr_col = pick_col(out, [\"HR\"])\n",
    "    sbp_col = pick_col(out, [\"SBP\"])\n",
    "    map_col = pick_col(out, [\"MAP\"])\n",
    "    bili_col = pick_col(out, [\"Bilirubin_total\", \"Bilirubin_direct\"])\n",
    "    creat_col = pick_col(out, [\"Creatinine\"])\n",
    "    plate_col = pick_col(out, [\"Platelets\"])\n",
    "\n",
    "    hr = safe_num(out, hr_col)\n",
    "    sbp = safe_num(out, sbp_col)\n",
    "    mapv = safe_num(out, map_col)\n",
    "    bili = safe_num(out, bili_col)\n",
    "    creat = safe_num(out, creat_col)\n",
    "    plate = safe_num(out, plate_col)\n",
    "\n",
    "    out[\"ShockIndex\"] = hr / (sbp.replace(0, np.nan))\n",
    "    out[\"BILI_CR\"] = bili / (creat.replace(0, np.nan))\n",
    "\n",
    "    # Partial SOFA-inspired aggregate\n",
    "    coag_pts = pd.cut(\n",
    "        plate,\n",
    "        bins=[-np.inf, 20, 50, 100, 150, np.inf],\n",
    "        labels=[4, 3, 2, 1, 0],\n",
    "    ).astype(float)\n",
    "    liver_pts = pd.cut(\n",
    "        bili,\n",
    "        bins=[-np.inf, 1.2, 2.0, 6.0, 12.0, np.inf],\n",
    "        labels=[0, 1, 2, 3, 4],\n",
    "    ).astype(float)\n",
    "    cardio_pts = pd.cut(\n",
    "        mapv,\n",
    "        bins=[-np.inf, 65, np.inf],\n",
    "        labels=[1, 0],\n",
    "    ).astype(float)\n",
    "    renal_pts = pd.cut(\n",
    "        creat,\n",
    "        bins=[-np.inf, 1.2, 2.0, 3.5, 5.0, np.inf],\n",
    "        labels=[0, 1, 2, 3, 4],\n",
    "    ).astype(float)\n",
    "\n",
    "    out[\"PartialSOFA\"] = coag_pts.fillna(0) + liver_pts.fillna(0) + cardio_pts.fillna(0) + renal_pts.fillna(0)\n",
    "    out[\"SOFA_Deterioration\"] = (\n",
    "        out.groupby(PATIENT_COL, sort=False)[\"PartialSOFA\"].diff().clip(lower=0).fillna(0)\n",
    "    )\n",
    "\n",
    "    roll_base = [c for c in [\"HR\", \"MAP\", \"SBP\", \"Resp\", \"O2Sat\", \"Temp\", \"Lactate\"] if c in out.columns]\n",
    "    for c in roll_base:\n",
    "        s = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "        out[f\"roll{LOOKBACK}_count_{c}\"] = (\n",
    "            s.notna().groupby(out[PATIENT_COL], sort=False).rolling(LOOKBACK, min_periods=1).sum().reset_index(level=0, drop=True)\n",
    "        )\n",
    "        out[f\"roll{LOOKBACK}_min_{c}\"] = (\n",
    "            s.groupby(out[PATIENT_COL], sort=False).rolling(LOOKBACK, min_periods=1).min().reset_index(level=0, drop=True)\n",
    "        )\n",
    "        out[f\"roll{LOOKBACK}_max_{c}\"] = (\n",
    "            s.groupby(out[PATIENT_COL], sort=False).rolling(LOOKBACK, min_periods=1).max().reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    return out\n",
    "\n",
    "def lead_lag_transform(X: np.ndarray) -> np.ndarray:\n",
    "    # (t, d) -> (2t-1, 2d)\n",
    "    if len(X) == 0:\n",
    "        return X\n",
    "    repeated = np.repeat(X, 2, axis=0)\n",
    "    lead = repeated[1:]\n",
    "    lag = repeated[:-1]\n",
    "    return np.hstack([lead, lag])\n",
    "\n",
    "def rolling_signature_block(df: pd.DataFrame, channels, lookback: int, order: int, prefix: str, use_cumsum: bool):\n",
    "    if not channels:\n",
    "        return pd.DataFrame(index=df.index)\n",
    "\n",
    "    x_all = df[channels].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    x_all_np = x_all.to_numpy(dtype=float)\n",
    "\n",
    "    dim0 = len(channels) * (2 if USE_LEAD_LAG else 1)\n",
    "    sig_len = iisignature.siglength(dim0, order)\n",
    "    out = np.zeros((len(df), sig_len), dtype=np.float32)\n",
    "\n",
    "    for pid, idx in df.groupby(PATIENT_COL, sort=False).groups.items():\n",
    "        idx = np.asarray(list(idx), dtype=int)\n",
    "        Xp = x_all_np[idx]\n",
    "        for j, row_idx in enumerate(idx):\n",
    "            start = max(0, j - lookback + 1)\n",
    "            window = Xp[start:j+1]\n",
    "            if use_cumsum:\n",
    "                window = np.cumsum(window, axis=0)\n",
    "            if USE_LEAD_LAG:\n",
    "                window = lead_lag_transform(window)\n",
    "            if window.shape[0] == 1:\n",
    "                window = np.vstack([window, window])\n",
    "            out[row_idx, :] = iisignature.sig(window, order)\n",
    "\n",
    "    cols = [f\"{prefix}_{k}\" for k in range(sig_len)]\n",
    "    return pd.DataFrame(out, columns=cols, index=df.index)\n",
    "\n",
    "def process_one_file(in_path: Path, out_path: Path):\n",
    "    if not in_path.exists():\n",
    "        raise FileNotFoundError(in_path)\n",
    "\n",
    "    df = pd.read_csv(in_path)\n",
    "    df = df.loc[:, ~df.columns.str.contains(r\"^Unnamed\")]\n",
    "    df = df.sort_values([PATIENT_COL, TIME_COL]).reset_index(drop=True)\n",
    "\n",
    "    base_cols = [c for c in df.columns if c not in {PATIENT_COL, TIME_COL, LABEL_COL}]\n",
    "\n",
    "    # build handcrafted block once\n",
    "    df_hc = add_handcrafted_features(df)\n",
    "    hc_cols = [c for c in df_hc.columns if c not in df.columns]\n",
    "\n",
    "    sig_channels = [c for c in [\"HR\", \"MAP\", \"SBP\", \"Resp\", \"O2Sat\", \"Temp\", \"Lactate\", \"Creatinine\", \"WBC\", \"Platelets\"] if c in df_hc.columns]\n",
    "    csig_channels = [c for c in [\"ICULOS\", \"HospAdmTime\", \"Creatinine\", \"BUN\", \"Lactate\", \"WBC\", \"Platelets\"] if c in df_hc.columns]\n",
    "\n",
    "    sig_df = rolling_signature_block(df_hc, sig_channels, LOOKBACK, SIG_ORDER, \"sig\", use_cumsum=False) if USE_SIGNATURE_VARS else pd.DataFrame(index=df_hc.index)\n",
    "    csig_df = rolling_signature_block(df_hc, csig_channels, LOOKBACK, CSIG_ORDER, \"csig\", use_cumsum=True) if USE_CUMSIGNATURE_VARS else pd.DataFrame(index=df_hc.index)\n",
    "\n",
    "    parts = [df_hc[[PATIENT_COL, TIME_COL]].copy()]\n",
    "    if LABEL_COL in df_hc.columns:\n",
    "        parts.append(df_hc[[LABEL_COL]])\n",
    "\n",
    "    if USE_ORIGINAL_VARS:\n",
    "        parts.append(df_hc[base_cols])\n",
    "    if USE_HANDCRAFTED_VARS and hc_cols:\n",
    "        parts.append(df_hc[hc_cols])\n",
    "    if USE_SIGNATURE_VARS:\n",
    "        parts.append(sig_df)\n",
    "    if USE_CUMSIGNATURE_VARS:\n",
    "        parts.append(csig_df)\n",
    "\n",
    "    final_df = pd.concat(parts, axis=1)\n",
    "    final_df.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"Saved: {out_path.name} | shape={final_df.shape} | base={len(base_cols)} hc={len(hc_cols)} sig={sig_df.shape[1]} csig={csig_df.shape[1]}\")\n",
    "\n",
    "for in_name, out_name in INPUT_FILES.items():\n",
    "    process_one_file(IN_DIR / in_name, OUT_DIR / out_name)\n",
    "\n",
    "print(\"\\nSignature feature run complete.\")\n",
    "print(\"IN_DIR:\", IN_DIR)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "print(\"RUN_DIR_NAME:\", RUN_DIR_NAME)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
